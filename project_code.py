# -*- coding: utf-8 -*-
"""DATA 301 Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IeNVu5nd2KzexD-ZU7fH1-Zumoj6xkdW
"""

import pandas as pd
import numpy as np
import re
import requests
from lxml import html
import json
from sklearn.neighbors import KNeighborsRegressor
import random
import math
from numpy.random import permutation

url = "https://raw.githubusercontent.com/fivethirtyeight/nba-player-advanced-metrics/master/nba-data-historical.csv"
nba = pd.read_csv(url, encoding='cp1252')
# The base data has question marks in place of diacritics

# 1977 to 2020, detailed stats per players
nba

# Creating a net rating stat
nba['NRtg'] = nba['ORtg'] - nba['DRtg']

# Filtering so that we're only using data from 2000 - 2020
nba = nba[nba['year_id']>=2000]

# Lots of names that have inconsistently filled with question marks instead of diacritics
# Real life data is not always clean
nba['name_common'][nba['name_common'].str.contains(pat = '\?')]

# Getting all nba teams from basketball references

p = requests.get('https://www.basketball-reference.com/awards/all_league.html')
tree = html.fromstring(p.content)

years = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/th/a[1]/text()')
league = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[1]/a[1]/text()')
team = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[2]/text()')
centers = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[3]/a[1]/text()')
forward1 = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[4]/a[1]/text()')
forward2 = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[5]/a[1]/text()')
guard1 = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[6]/a[1]/text()')
guard2 = tree.xpath('//*[@id="awards_all_league"]/tbody/tr/td[7]/a[1]/text()')

df = pd.DataFrame({'Year':years, 'team':team, 'centers':centers, 'forward1':forward1, 'forward2':forward2, 'guard1':guard1, 'guard2':guard2})
df['Year'] = pd.to_numeric(df['Year'].str.slice(stop=4))+1
df = df[df['Year']>=2000]
stacked = pd.melt(df, id_vars='Year', value_vars=['centers', 'forward1', 'forward2', 'guard1', 'guard2'], value_name='players').sort_values(by='Year', ascending=False)
stacked.columns = ['Year', 'Position', 'players']
stacked

# The two datasets we are using are not consistent with how they are dealing with diacritics
# Solution is to manually clean them up since they are not even consistent with how they deal with it within their datasets

stacked['players'][stacked['players'].str.contains(pat = 'Luka Dončić')] = 'Luka Don?i?'
stacked['players'][stacked['players'].str.contains(pat = 'Nikola Jokić')] = 'Nikola Joki?'
stacked['players'][stacked['players'].str.contains(pat = 'Manu Ginóbili')] = 'Manu Ginobili'
stacked['players'][stacked['players'].str.contains(pat = 'Peja Stojaković')] = 'Peja Stojakovic'
stacked['players'][stacked['players'].str.contains(pat = 'Goran Dragić')] = 'Goran Dragic'

# We add the 'all NBA' column here to indicate which players are all NBA

allnba = pd.merge(nba, stacked, how='left', left_on=['name_common', 'year_id'], right_on=['players', 'Year'])
allnba['All NBA'] = allnba['Position'].notnull()
allnba['All NBA'] = allnba['All NBA'].astype(int)
allnba[allnba['All NBA'] == 1]

# The non all-nba players
allnba[allnba['All NBA'] == 0]

# We find the correlation between all NBA and their player stats here 
allnba[allnba.columns[8:41]].apply(lambda x: x.corr(allnba['All NBA'])).sort_values()
## Top correlated stats with allnba status: ['P/36','MP%','%Pos', 'Min', 'PIE%', 'USG%']

# Commented out IPython magic to ensure Python compatibility.
# Plotting the difference in player stats between all NBA and non all NBA players

import matplotlib.pyplot as plt
# %matplotlib inline

dcols = ['P/36','MP%','%Pos', 'Min', 'PIE%', 'USG%']

fig, axes = plt.subplots(2, 3, figsize=(14, 10), sharex=False, sharey=False)
for ax, col in zip(axes.flatten(), dcols):
  cat = allnba.groupby('All NBA')[col].mean()
  cat.plot.bar(ax=ax)
  ax.set_title(col)

import altair as alt
import matplotlib

dcols = ['P/36','MP%','%Pos', 'Min', 'PIE%', 'USG%']

fig, axes = plt.subplots(2, 3, figsize=(20, 10), sharex=False, sharey=False)
for ax, col in zip(axes.flatten(), dcols):
  cat = allnba.groupby(['All NBA','year_id'])[col].mean().reset_index()
  all_n = cat[cat['All NBA'] == 1]
  non_all_n = cat[cat['All NBA'] == 0]
  all_n.plot.scatter(x='year_id', y=col, c='Green', label='All NBA', ax=ax)
  non_all_n.plot.scatter(x='year_id', y=col, c='Red', label='Not All NBA', ax=ax)
  ax.set_title(col)
  ax.set_xlabel('Year')
  ax.xaxis.set_major_locator(plt.MultipleLocator(5))

def normalize(df, columns):
  result = df[columns]
  result = (result - result.mean())/result.std()
  return result

# We tried to use PCA however our results were not satisfactory 

from sklearn.decomposition import PCA

cols = ['P/36','MP%','%Pos', 'Min', 'PIE%', 'USG%', 'All NBA']

normalized_allnba = normalize(allnba, cols)
df_temp = normalized_allnba[cols]
df_temp.fillna(0, inplace=True)

pca = PCA(n_components=6)
pca.fit(df_temp)
df_pca = pca.transform(df_temp)
df_pca = pd.DataFrame(df_pca)

df_pca.plot.scatter(x = 0, y = 1)

# Creating Balanced Data
# Selecting 317 random non-all nba players

non = allnba[allnba['All NBA']==0] # separating non all-nba
all = allnba[allnba['All NBA']==1] # separating all-nba
non_random_indices = permutation(non.index)
non = non.loc[non_random_indices[1:318]] # Pulling random non all-nba players
balanced = pd.concat([all,non]) # Putting the data back together
balanced

cols = ['P/36','MP%','%Pos', 'Min', 'PIE%', 'USG%', 'All NBA']

normalized_balanced = normalize(balanced, cols)
df_temp = normalized_balanced[cols]
df_temp.fillna(0, inplace=True)

pca = PCA(n_components=2)
pca.fit(df_temp)
df_pca = pca.transform(df_temp)
df_pca = pd.DataFrame(df_pca)

df_pca.plot.scatter(x=0,y=1)

# Credit to: https://towardsdatascience.com/knn-visualization-in-just-13-lines-of-code-32820d72c6b6
# We use this function to show how KNN does on our data

import matplotlib.pyplot as plt
import pandas as pd
from sklearn import datasets, neighbors
from mlxtend.plotting import plot_decision_regions

def knn_comparison(data, k):
    x = data[['P/36','MP%']].values
    y = data['All NBA'].astype(int).values
    clf = neighbors.KNeighborsClassifier(n_neighbors=k)
    clf.fit(x, y)
    
    # Plotting decision region
    plot_decision_regions(x, y, clf=clf, legend=2)

    # Adding axes annotations
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.title('Knn with K='+ str(k))
    plt.show()

"""KNN Unbalanced"""

allnba.fillna(0, inplace=True)
balanced.fillna(0, inplace=True)
x_columns = ['P/36','MP%','%Pos','Min','PIE%', 'USG%']
y_column = ['All NBA']

random_indices = permutation(allnba.index)
test_cutoff = math.floor(len(allnba)/3)
test = allnba.loc[random_indices[1:test_cutoff]]
train = allnba.loc[random_indices[test_cutoff:]]

unbalanced_knn = KNeighborsRegressor(n_neighbors=5)

training_normalized = normalize(train, x_columns)
testing_normalized = normalize(test, x_columns)

unbalanced_knn.fit(training_normalized[x_columns], train[y_column])
predictions = unbalanced_knn.predict(testing_normalized[x_columns])

actual = test[y_column]
error = (abs(predictions - actual)).sum() / len(predictions)

error

allnbacol = test['All NBA']
testing_normalized["All NBA"] = allnbacol 

knn_comparison(testing_normalized, 5)

"""KNN Balanced"""

bal_random_indices = permutation(balanced.index)
bal_test_cutoff = math.floor(len(balanced)/3)
bal_test = balanced.loc[bal_random_indices[1:bal_test_cutoff]]
bal_train = balanced.loc[bal_random_indices[bal_test_cutoff:]]

bal_knn = KNeighborsRegressor(n_neighbors=5)

bal_training_normalized = normalize(bal_train, x_columns)
bal_testing_normalized = normalize(bal_test, x_columns)

bal_knn.fit(bal_training_normalized[x_columns], bal_train[y_column])
bal_predictions = bal_knn.predict(bal_testing_normalized[x_columns])

bal_actual = bal_test[y_column]
bal_error = (abs(bal_predictions - bal_actual)).sum() / len(bal_predictions)

bal_error

allnbacol = balanced['All NBA']
norm_balanced = normalize(balanced, x_columns)
norm_balanced["All NBA"] = allnbacol

knn_comparison(norm_balanced, 5)

knn_comparison(balanced, 5)

"""KNN Unbalanced training but testing on balanced data to see how accurate the model actually is"""

random_indices = permutation(allnba.index)
test_cutoff = math.floor(len(allnba)/3)
test = allnba.loc[random_indices[1:test_cutoff]]
train = allnba.loc[random_indices[test_cutoff:]]

unbalanced_knn = KNeighborsRegressor(n_neighbors=5)

training_normalized = normalize(train, x_columns)
testing_normalized = normalize(test, x_columns)

unbalanced_knn.fit(training_normalized[x_columns], train[y_column])
predictions2 = unbalanced_knn.predict(bal_testing_normalized[x_columns])

bal_actual2 = bal_test[y_column]
bal_error2 = (abs(predictions2 - bal_actual2)).sum() / len(predictions2)

bal_error2

"""Neural Network Implementation

Unbalanced Data
"""

test_x = test.loc[:,test.columns != 'All NBA'][x_columns]
test_y = test.iloc[:,-1]
train_x = train.loc[:,train.columns != 'All NBA'][x_columns]
train_y = train.iloc[:,-1]

from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from sklearn.utils import class_weight

class_weight = class_weight.compute_class_weight('balanced'
                                               ,np.unique(train_y)
                                               ,train_y)

normalized_train_x = normalize(train_x, x_columns)
normalized_test_x = normalize(test_x, x_columns)

model1 = models.Sequential()
model1.add(layers.Dense(10,activation='relu', 
					input_shape=(6,)))
model1.add(layers.Dense(8,activation='relu'))
model1.add(layers.Dense(5,activation='relu'))
model1.add(layers.Dense(1, activation='relu'))
model1.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model1.fit(x=normalized_train_x, y=train_y, epochs=4, batch_size = 20, verbose=1, class_weight = {0:class_weight[0], 1:class_weight[1]})

results = model1.evaluate(normalized_test_x, test_y)
results[1]

class_weight

#Predictions using the Unbalanced NN Model

#player input stats: 'P/36','MP%','%Pos','Min','PIE%', 'USG%'
#stats in terms of z-scores

player1 = [[-2, -2, -2, -2, -2, -2]]
player2 = [[-1, -1, -1, -1, -1, -1]]
player3 = [[0, 0, 0, 0, 0, 0]]
player4 = [[1, 1, 1, 1, 1, 1]]
player5 = [[2, 2, 2, 2, 2, 2]]

model1.predict_classes([player1, player2, player3, player4, player5])

"""Balanced Data"""

bal_test_x = bal_test.loc[:,bal_test.columns != 'All NBA'][x_columns]
bal_test_y = bal_test.iloc[:,-1]
bal_train_x = bal_train.loc[:,bal_train.columns != 'All NBA'][x_columns]
bal_train_y = bal_train.iloc[:,-1]

from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from sklearn.utils import class_weight

class_weight = class_weight.compute_class_weight('balanced'
                                               ,np.unique(bal_train_y)
                                               ,bal_train_y)

norm_bal_train_x = normalize(bal_train_x, x_columns)
norm_bal_test_x = normalize(bal_test_x, x_columns)

model = models.Sequential()
model.add(layers.Dense(10,activation='relu', 
					input_shape=(6,)))
model.add(layers.Dense(10,activation='relu'))
model.add(layers.Dense(10,activation='relu'))
model.add(layers.Dense(1, activation='relu'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit(x=norm_bal_train_x, y=bal_train_y, epochs=4, batch_size = 5, verbose=1, class_weight = {0:class_weight[0], 1:class_weight[1]})

results = model.evaluate(norm_bal_test_x, bal_test_y)
results[1]

class_weight

#Predictions using the Balanced NN Model

#player input stats: 'P/36','MP%','%Pos','Min','PIE%', 'USG%'
#stats in terms of z-scores

player1 = [[-2, -2, -2, -2, -2, -2]]
player2 = [[-1, -1, -1, -1, -1, -1]]
player3 = [[0, 0, 0, 0, 0, 0]]
player4 = [[1, 1, 1, 1, 1, 1]]
player5 = [[2, 2, 2, 2, 2, 2]]

model.predict_classes([player1, player2, player3, player4, player5])

"""NN Unbalanced training but testing on balanced data to see how accurate the model actually is"""

from tensorflow.python.keras import models
from tensorflow.python.keras import layers
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix
from sklearn.utils import class_weight

class_weight = class_weight.compute_class_weight('balanced'
                                               ,np.unique(train_y)
                                               ,train_y)

normalized_train_x = normalize(train_x, x_columns)
normalized_test_x = normalize(test_x, x_columns)

model = models.Sequential()
model.add(layers.Dense(10,activation='relu', 
					input_shape=(6,)))
model.add(layers.Dense(8,activation='relu'))
model.add(layers.Dense(5,activation='relu'))
model.add(layers.Dense(1, activation='relu'))
model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])
model.fit(x=normalized_train_x, y=train_y, epochs=4, batch_size = 20, verbose=1, class_weight = {0:class_weight[0], 1:class_weight[1]})

results = model.evaluate(norm_bal_test_x, bal_test_y)
results[1]